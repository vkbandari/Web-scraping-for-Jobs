{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all necessary libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from math import ceil\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#=======================================================================================================\n",
    "\n",
    "# array of internshala links\n",
    "links_list = ['https://internshala.com/internships/work-from-home-jobs', \\\n",
    "            'https://internshala.com/internships/internship-in-bangalore', \\\n",
    "            'https://internshala.com/internships/internship-in-hyderabad', \\\n",
    "            'https://internshala.com/internships/internship-in-odisha']\n",
    "\n",
    "#=======================================================================================================\n",
    "\n",
    "\n",
    "def scrape_main(link):\n",
    "    '''\n",
    "     a function that request the webpage and store it in response object.\n",
    "\n",
    "     then passing lxml parser to parse over the webpage.\n",
    "\n",
    "     here lxml parser defines the speed to parse the webpage.\n",
    "\n",
    "     if it throws any errors in using lxml parser. just install it by: \" pip install lxml \"\n",
    "    '''\n",
    "\n",
    "    response = requests.get(link)\n",
    "    return (BeautifulSoup(response.text, 'lxml'))\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "# timestamp as file name by using time library and with prefix as internshala\n",
    "file_name_1 = 'internshala_first_data_' + time.strftime(\"%d_%m_%Y_%H_%M_%S\") + \".csv\"\n",
    "\n",
    "\n",
    "#============================================================================================================\n",
    "\n",
    "def get_links_internshala(links = links_list):\n",
    "    '''\n",
    "        Function to extract unique links of job postings in internshala.\n",
    "    '''\n",
    "\n",
    "    # generalied locations based on links and its position\n",
    "    loc_links = ['work_from_home', 'bangalore', 'hyderabad', 'odisha']\n",
    "\n",
    "    # writing row heading to understand each column\n",
    "    row_heading = ['source', 'location', 'job_link']\n",
    "\n",
    "    # opening file in write mode and connecting csv writer to file\n",
    "    file = open(file_name_1, 'w')\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # initially writing header of csv file\n",
    "    writer.writerow(row_heading)\n",
    "\n",
    "    # looping over array of links with index value\n",
    "    for index, url in enumerate(links):\n",
    "        # passing main page to scape\n",
    "        soup = scrape_main(url)\n",
    "\n",
    "        #  to find number of job pages to scrape we need to get count of jobs available which is at heading in webpage.\n",
    "        # print(soup.find('div',{'class':'heading heading_4_6'}))\n",
    "\n",
    "        # based on count of jobs - finding the number of pages available at one link in array of links.\n",
    "        pages = ceil(int(soup.find('div', {'class': 'heading heading_4_6'}).text.split()[0]) / 40)\n",
    "        for page in range(pages):\n",
    "            # now we need to scrape over pages under main url\n",
    "            base_url = url + \"/page-\" + str(page)\n",
    "            soup1 = scrape_main(base_url)\n",
    "\n",
    "            # firstly finding each single job in each page to find job link\n",
    "            for single_job in soup.find_all(\"div\", {\"class\": \"individual_internship\"}):\n",
    "\n",
    "                if (single_job.find('div', {'class': 'heading_4_5 profile'}) == None):\n",
    "                    continue\n",
    "\n",
    "                job_link = \"https://internshala.com\"\n",
    "                job_link += single_job.find('div', {'class': 'heading_4_5 profile'}).a.get('href')\n",
    "\n",
    "                source = 'internshala'\n",
    "\n",
    "                location = loc_links[index]\n",
    "\n",
    "                # writing all details to csv\n",
    "                writer.writerow([source, location, job_link])\n",
    "                break\n",
    "            break\n",
    "\n",
    "    # closing csv file\n",
    "    file.close()\n",
    "\n",
    "    # reading csv file\n",
    "    df = pd.read_csv(file_name_1)\n",
    "\n",
    "    # df.tail(5)\n",
    "\n",
    "    # size of jobs collected\n",
    "    # df.shape\n",
    "\n",
    "    # removing extracted csv file\n",
    "    os.remove(file_name_1)\n",
    "\n",
    "    # storing to csv file\n",
    "    df.to_csv(file_name_1, index=False)\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "# timestamp as file name by using time library and with prefix as internshala\n",
    "file_name_2 = 'internshala_second_data_' + time.strftime(\"%d_%m_%Y_%H_%M_%S\") + \".csv\"\n",
    "\n",
    "\n",
    "#===============================================================================================================\n",
    "\n",
    "def get_complete_info_internshala(file_name = file_name_1):\n",
    "    '''\n",
    "        A function in internshala scraping to extract all the information about each job posting based on link.\n",
    "\n",
    "    '''\n",
    "    # reading first csv file which contains the links\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    # checking 5 job records\n",
    "    #df.head(5)\n",
    "\n",
    "    # writing row heading to understand each column\n",
    "    row_heading = ['source', 'location', 'job_link', 'job_title', \\\n",
    "                   'company_name', 'imp_fields', 'description_headings', 'description']\n",
    "\n",
    "\n",
    "    # opening file in write mode and connecting csv writer to file\n",
    "    file = open(file_name_2, 'w')\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # initially writing header of csv file\n",
    "    writer.writerow(row_heading)\n",
    "\n",
    "    # looping over dataframe link column with index value to give job location and source to new data frame\n",
    "    for index, link in enumerate(df.job_link):\n",
    "        # passing  url to scrape on each job link\n",
    "        soup = scrape_main(link)\n",
    "\n",
    "        # getting job title by finding unique class name in webpage\n",
    "        if soup.find('div', {'class': 'heading_4_5 profile'}) == None:\n",
    "            continue\n",
    "\n",
    "        job_title = soup.find('div', {'class': 'heading_4_5 profile'}).text.strip()\n",
    "\n",
    "        # getting company name by unique class name of div tag\n",
    "        company_name = soup.find('div', {'class': 'heading_6 company_name'}).text.strip()\n",
    "\n",
    "        '''\n",
    "        # getting important fields in the job posting as list\n",
    "        fields are:\n",
    "        1. start date of joining/mode of vacancy\n",
    "        2. duration of job\n",
    "        3. incentives/stipend\n",
    "        4. last date to apply\n",
    "        5. types of doing job \n",
    "        '''\n",
    "\n",
    "        imp_fields = []\n",
    "        for i in soup.find_all('div', {'class': 'item_body'}):\n",
    "            imp_fields.append(i.get_text().strip())\n",
    "\n",
    "        # this list is extracting for further process to do in description to get valuable information.\n",
    "        description_headings = []\n",
    "        for i in soup.find_all('div', {'class': 'section_heading heading_5_5'}):\n",
    "            description_headings.append(i.get_text().strip())\n",
    "\n",
    "        # complete description of job\n",
    "        description = soup.find('div', {'class': 'internship_details'}).get_text().strip()\n",
    "\n",
    "        # writing to the server\n",
    "        writer.writerow([df.source[index], df.location[index], df.job_link[index], \\\n",
    "                         job_title, company_name, imp_fields, description_headings, description])\n",
    "\n",
    "    # closing file object\n",
    "    file.close()\n",
    "\n",
    "    # loading extracted csv file to the dataframe\n",
    "    df2 = pd.read_csv(file_name_2)\n",
    "\n",
    "    #df2.sample(5)\n",
    "\n",
    "    # getting size of csv file\n",
    "    #df2.shape\n",
    "\n",
    "    #removing extracted csv file\n",
    "    os.remove(file_name_2)\n",
    "\n",
    "    #### To save it into normal csv file without spaces\n",
    "    df2.to_csv(file_name_2, index = False)\n",
    "\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "def to_database_format_internshala():\n",
    "    '''\n",
    "        To save file as pipe '|' as delimiter\n",
    "    '''\n",
    "    print(\"\\tHere the csv file stored in pipe as delimiter format \" \\\n",
    "            \"\\n\\t file names are given in time format way as: \" \\\n",
    "          \"\\n\\t'internshala_first/second_database_ + time.strftime(%d_%m_%Y_%H_%M_%S)'\")\n",
    "\n",
    "    # reading first csv file\n",
    "    df_1 = pd.read_csv(file_name_1)\n",
    "\n",
    "    # given time stamp file\n",
    "    file_db_1 = 'internshala_first_database_' + time.strftime(\"%d_%m_%Y_%H_%M_%S\") + '.csv'\n",
    "\n",
    "    # saving file as db format\n",
    "    df_1.to_csv(file_db_1, sep='|', index = False)\n",
    "\n",
    "    # reading second csv file\n",
    "    df_2 = pd.read_csv(file_name_2)\n",
    "\n",
    "    # given time stamp file\n",
    "    file_db_2 = 'internshala_second_database_' + time.strftime(\"%d_%m_%Y_%H_%M_%S\") + '.csv'\n",
    "\n",
    "    # saving file as db format\n",
    "    df_2.to_csv(file_db_2, sep='|', index = False)\n",
    "\n",
    "\n",
    "\n",
    "#============================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_links_internshala()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_complete_info_internshala()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tHere the csv file stored in pipe as delimiter format \n",
      "\t file names are given in time format way as: \n",
      "\t'internshala_first/second_database_ + time.strftime(%d_%m_%Y_%H_%M_%S)'\n"
     ]
    }
   ],
   "source": [
    "to_database_format_internshala()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "#========================================================================================================\n",
    "\n",
    "# array of indeed links\n",
    "links_list = ['https://www.indeed.co.in/jobs?q=&l=Telangana&radius=100&sort=date&start=', \\\n",
    "              'https://www.indeed.co.in/jobs?q=&l=Karnataka&radius=100&sort=date&start=', \\\n",
    "              'https://www.indeed.co.in/jobs?q=&l=Orissa&radius=100&sort=date&start=']\n",
    "\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "def url_soup(url):\n",
    "    '''\n",
    "      A function that request the webpage and store it in response object.\n",
    "\n",
    "      Then passing lxml parser to parse over the webpage.\n",
    "\n",
    "      Here lxml parser defines the speed to parse the webpage.\n",
    "\n",
    "      If it throws any errors in using lxml parser. just install it by: \" pip install lxml \"\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    return (BeautifulSoup(response.text, 'lxml'))\n",
    "\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "# timestamp as file name by using time library and with prefix as indeed\n",
    "file_name = \"indeed_\" + time.strftime(\"%d_%m_%Y_%H_%M_%S\") + \".csv\"\n",
    "\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "def scrape_indeed(list = links_list):\n",
    "    '''\n",
    "        A function in indeed to extract jobs in indeed using the array of links_list,\n",
    "\n",
    "        where all job links are given based on location wise.\n",
    "    '''\n",
    "\n",
    "    # writing row heading to understand each column\n",
    "    row_heading = ['Source', 'job_title', 'company_name', 'salary', 'location', 'short_summary', 'link']\n",
    "\n",
    "    # opening file in write mode and connecting csv writer to file\n",
    "    file = open(file_name, 'w')\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # initially writing header of csv file\n",
    "    writer.writerow(row_heading)\n",
    "\n",
    "    # looping over array of links with index value\n",
    "    for i in list:\n",
    "        sp = url_soup(i)\n",
    "        cont = sp.find(\"div\", {\"id\": \"searchCountPages\"})\n",
    "        jobs = cont.string.split()[3]\n",
    "        jobs = jobs.replace(',', '')\n",
    "        for page in range(0, 100, 10):\n",
    "            container = sp.findAll(\"div\", {\"class\": \"jobsearch-SerpJobCard\"})\n",
    "            # print(len(container))\n",
    "\n",
    "            for each_job in range(len(container)):\n",
    "                source = \"indeed\"\n",
    "\n",
    "                # getting single job posting work title\n",
    "                job_title = container[each_job].find('a', {'class': 'jobtitle'}).string.strip()\n",
    "\n",
    "                # getting single job company name\n",
    "                comp_na = container[each_job].find('span', {'class': 'company'}).string\n",
    "                if comp_na != None:\n",
    "                    comp_name = comp_na.strip()\n",
    "                else:\n",
    "                    comp_name = None\n",
    "\n",
    "                # getting single job salary\n",
    "                sal = container[each_job].find('span', {'class': 'salaryText'})\n",
    "                if sal != None:\n",
    "                    salary = sal.string.strip()\n",
    "                else:\n",
    "                    salary = None\n",
    "\n",
    "                # getting single job location\n",
    "                job_lo = container[each_job].find('div', {'class': 'location'})\n",
    "                if job_lo != None:\n",
    "                    job_loc = job_lo.string.strip()\n",
    "                else:\n",
    "                    job_loc = None\n",
    "\n",
    "                # getting single job summary\n",
    "                job_short_summa = container[each_job].find('div', {'class': 'summary'}).li\n",
    "                if job_short_summa != None:\n",
    "                    job_short_summary = job_short_summa.string.strip()\n",
    "                else:\n",
    "                    job_short_summary = None\n",
    "\n",
    "                # to get complete info\n",
    "                # getting single job posting link\n",
    "                link = 'https://www.indeed.co.in'\n",
    "                link += container[each_job].a.get('href')\n",
    "                job_link = link\n",
    "                lis = [source, job_title, comp_name, salary, job_loc, job_short_summary, job_link]\n",
    "                # for i in lis:\n",
    "                # print(i)\n",
    "\n",
    "                # writing all details to csv\n",
    "                writer.writerow(lis)\n",
    "                break\n",
    "            break\n",
    "    # closing csv file\n",
    "    file.close()\n",
    "\n",
    "    # reading csv file\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    # df.tail(5)\n",
    "\n",
    "    # size of jobs collected\n",
    "    # df.shape\n",
    "\n",
    "    # removing extracted csv file\n",
    "    os.remove(file_name)\n",
    "\n",
    "    # storing to csv file\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "def to_database_format_indeed():\n",
    "    '''\n",
    "        To save file as pipe '|' as delimiter\n",
    "    '''\n",
    "    print(\"\\tHere the csv file stored in pipe as delimiter format \" \\\n",
    "            \"\\n\\t file name is given in time format way as: \" \\\n",
    "          \"\\n\\t'indeed_database_ + time.strftime(%d_%m_%Y_%H_%M_%S)'\")\n",
    "\n",
    "    # reading first csv file\n",
    "    df_1 = pd.read_csv(file_name)\n",
    "\n",
    "    # given time stamp file\n",
    "    file_db_1 = 'indeed_database_' + time.strftime(\"%d_%m_%Y_%H_%M_%S\") + '.csv'\n",
    "\n",
    "    # saving file as db format\n",
    "    df_1.to_csv(file_db_1, sep='|', index = False)\n",
    "\n",
    "\n",
    "# =================================================================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_indeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tHere the csv file stored in pipe as delimiter format \n",
      "\t file name is given in time format way as: \n",
      "\t'indeed_database_ + time.strftime(%d_%m_%Y_%H_%M_%S)'\n"
     ]
    }
   ],
   "source": [
    "to_database_format_indeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all necessary libraries\n",
    "\n",
    "from facebook_scraper_lib import get_posts\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "def url_rq(link):\n",
    "    '''\n",
    "      A function that request the webpage and store it in response object.\n",
    "\n",
    "      Then passing lxml parser to parse over the webpage.\n",
    "\n",
    "      Here lxml parser defines the speed to parse the webpage.\n",
    "\n",
    "      If it throws any errors in using lxml parser. just install it by: \" pip install lxml \"\n",
    "    '''\n",
    "    response = requests.get(link)\n",
    "    sp = soup(response.text, 'lxml')\n",
    "    return(sp.find('div',{'dir':'ltr'}))\n",
    "\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "\n",
    "# timestamp as file name by using time library and with prefix as facebook\n",
    "file_name = \"facebook_\" + time.strftime(\"%d_%m_%Y_%H_%M_%S\") + \".csv\"\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "def scrape_facebook():\n",
    "    '''\n",
    "        A function to get the posts in facebook and based on links in facebook it going to rescrape on that link.\n",
    "\n",
    "        here it works only on links of id : 380555718642309.\n",
    "\n",
    "        since the id values of other websites of other id might be different so need to cross check.\n",
    "    '''\n",
    "\n",
    "    # writing row heading to understand each column\n",
    "    row_heading = ['post_id', 'text', 'post_text', 'shared_text', 'time', \\\n",
    "                   'likes', 'comments', 'shares', 'link', 'jobs_info']\n",
    "\n",
    "    # opening file in write mode and connecting csv writer to file\n",
    "    with open(file_name, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # initially writing header of csv file\n",
    "        writer.writerow(row_heading)\n",
    "\n",
    "        # extracting posts from facebook  by using facebook_scraper_lib\n",
    "        for post in get_posts('380555718642309', pages=1):\n",
    "\n",
    "            # for jobs_info the actual scraping is done to extract xml data of multiple jobs.\n",
    "            jobs_info = url_rq(post['link'])\n",
    "\n",
    "            # writing all details to csv\n",
    "            writer.writerow([post['post_id'], post['text'], post['post_text'],\\\n",
    "                             post['shared_text'], post['time'], post['likes'], post['comments'],\\\n",
    "                             post['shares'], post['link'], jobs_info])\n",
    "\n",
    "\n",
    "    df = pd.read_csv(file_name)\n",
    "    # df.tail(5)\n",
    "\n",
    "    # size of jobs collected\n",
    "    # df.shape\n",
    "\n",
    "    # removing extracted csv file\n",
    "    os.remove(file_name)\n",
    "\n",
    "    # storing to csv file\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "\n",
    "\n",
    "#=================================================================================================================\n",
    "\n",
    "def to_database_format_facebook():\n",
    "    '''\n",
    "        To save file as pipe '|' as delimiter\n",
    "    '''\n",
    "    print(\"\\tHere the csv file stored in pipe as delimiter format \" \\\n",
    "            \"\\n\\t file name is given in time format way as: \" \\\n",
    "          \"\\n\\t'facebook_database_ + time.strftime(%d_%m_%Y_%H_%M_%S)'\")\n",
    "\n",
    "    # reading first csv file\n",
    "    df_1 = pd.read_csv(file_name)\n",
    "\n",
    "    # given time stamp file\n",
    "    file_db_1 = 'facebook_database_' + time.strftime(\"%d_%m_%Y_%H_%M_%S\") + '.csv'\n",
    "\n",
    "    # saving file as db format\n",
    "    df_1.to_csv(file_db_1, sep='|', index = False)\n",
    "\n",
    "\n",
    "# =================================================================================================================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_facebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tHere the csv file stored in pipe as delimiter format \n",
      "\t file name is given in time format way as: \n",
      "\t'facebook_database_ + time.strftime(%d_%m_%Y_%H_%M_%S)'\n"
     ]
    }
   ],
   "source": [
    "to_database_format_facebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
